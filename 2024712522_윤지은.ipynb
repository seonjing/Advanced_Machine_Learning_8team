{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 2024 기계학습특론 _ 윤지은) Style-Based Transformer for Time Series Forecasting 코드 구현\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 라이브러리 임포트\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 클래스 정의 - 데이터 전처리\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(\n",
    "        self, path, sequence_len=30, output_len=30, sliding_window=1, client=None\n",
    "    ):\n",
    "        self.sequence_len = sequence_len\n",
    "        self.output_len = output_len\n",
    "        self.sliding_window = sliding_window\n",
    "\n",
    "        # 데이터 로드 및 전처리\n",
    "        data = pd.read_csv(path, sep=\";\", decimal=\",\")\n",
    "        data = data.iloc[35040:, 1:322] # n_clients = 321 & 0 제거\n",
    "        data = data.groupby(data.index // 4).sum() # 논문의 electricity 참조 참고 (1시간 간격으로 합산해 계산)\n",
    "        data = torch.FloatTensor(data.to_numpy())  # (total_seq_len, feature_len)\n",
    "\n",
    "        \n",
    "\n",
    "        if client is not None:\n",
    "            data = data[:, client : client + 1]\n",
    "\n",
    "        # 데이터 정규화\n",
    "        scaler = MinMaxScaler()\n",
    "        data = torch.FloatTensor(scaler.fit_transform(data))\n",
    "\n",
    "        # 나중에 역변환을 위해 scaler 저장\n",
    "        self.scaler = scaler\n",
    "\n",
    "        total_seq_len, feature_len = data.shape\n",
    "        self.feature_len = feature_len\n",
    "\n",
    "        # sliding window를 사용하여 데이터 생성\n",
    "        x_indices = []\n",
    "        y_indices = []\n",
    "\n",
    "        for i in range(\n",
    "            0, total_seq_len - sequence_len - output_len + 1, sliding_window\n",
    "        ):\n",
    "            x_indices.append(slice(i, i + sequence_len))\n",
    "            y_indices.append(slice(i + sequence_len, i + sequence_len + output_len))\n",
    "\n",
    "        # 데이터 분리\n",
    "        self.data_x = torch.stack(\n",
    "            [data[x_idx] for x_idx in x_indices]\n",
    "        )  # (data_size, sequence_len, feature_len)\n",
    "        self.data_y = torch.stack(\n",
    "            [data[y_idx] for y_idx in y_indices]\n",
    "        )  # (data_size, output_len, feature_len)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data_x[idx], self.data_y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 디코더에 들어갈 latent vector (초기 시계열 순차)를 구하는 함수 - 논문 eq.8\n",
    "def calculate_linear_regression_parallel(x):\n",
    "    # x: (sequence_len, batch_size, feature_len)\n",
    "\n",
    "    m = x.shape[0]\n",
    "    x_mean = x.sum(dim=0, keepdim=False) / m  # (batch_size, feature_len)\n",
    "\n",
    "    # 기울기 계산\n",
    "    numerator = torch.zeros_like(x_mean)  # (batch_size, feature_len)\n",
    "    for i in range(m):\n",
    "        numerator += (i + 1 - m / 2) * (\n",
    "            x[i, :, :] - x_mean\n",
    "        )  # (batch_size, feature_len)\n",
    "\n",
    "    # 최소 제곱법에 의한 기울기 계산\n",
    "    a = ((12 / (m**3 + 2 * m)) * numerator).unsqueeze(0)  # (1, batch_size, feature_len)\n",
    "\n",
    "    # y절편 계산\n",
    "    b = x_mean.unsqueeze(0) - a * (m / 2)  # (1, batch_size, feature_len)\n",
    "\n",
    "    return (x * a) + b  # (sequence_len, batch_size, feature_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# positional encoding 모듈 정의\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=500, device=\"cuda\"):\n",
    "        super().__init__()\n",
    "\n",
    "        self.PE = torch.zeros(max_len, d_model, device=device)\n",
    "        self.PE.requires_grad = False\n",
    "\n",
    "        pos = torch.arange(0, max_len, dtype=torch.float, device=device).unsqueeze(\n",
    "            dim=1\n",
    "        )\n",
    "        interval = torch.arange(0, d_model, step=2, dtype=torch.float, device=device)\n",
    "\n",
    "        self.PE[:, 0::2] = torch.sin(pos / 10000 ** (interval / d_model))\n",
    "        self.PE[:, 1::2] = torch.cos(pos / 10000 ** (interval / d_model))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (sequence_len, batch_size, feature_len)\n",
    "        sequence_len, batch_size, feature_len = x.shape\n",
    "        return x + self.PE[:sequence_len, :].unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AdaIN (Adaptive Instance Normalization) - 논문 eq.9\n",
    "class AdaIN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x, style):\n",
    "        # (output_seq_len:n, batch_size, feature_len), (sequence_len, batch_size, 1)\n",
    "\n",
    "        x_mean = torch.mean(x, dim=0, keepdim=True)\n",
    "        x_std = torch.std(x, dim=0, keepdim=True) + 1e-7\n",
    "        style_mean = torch.mean(style, dim=0, keepdim=True)\n",
    "        style_std = torch.std(style, dim=0, keepdim=True) + 1e-7\n",
    "\n",
    "        # (output_seq_len:n, batch_size, feature_len)\n",
    "        normalized_content = (x - x_mean) / x_std\n",
    "        return normalized_content * style_std + style_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인코더 모듈 정의\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, feature_len):\n",
    "        super().__init__()\n",
    "\n",
    "        d_model = 4  # 논문 고정 값.\n",
    "\n",
    "        self.input_projection = nn.Linear(feature_len, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model)\n",
    "\n",
    "        # 첫 번째 레이어 (d_model -> d_model//2)\n",
    "        encoder_layer1 = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, nhead=2  # nhead는 d_model의 약수여야 함\n",
    "        )\n",
    "        self.transformer_encoder1 = nn.TransformerEncoder(encoder_layer1, num_layers=1)\n",
    "        self.projection1 = nn.Linear(d_model, d_model // 2)\n",
    "\n",
    "        # 두 번째 레이어 (d_model//2 -> d_model//4 == 1)\n",
    "        encoder_layer2 = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model // 2, nhead=1  # nhead는 d_model의 약수여야 함\n",
    "        )\n",
    "        self.transformer_encoder2 = nn.TransformerEncoder(encoder_layer2, num_layers=1)\n",
    "        self.projection2 = nn.Linear(d_model // 2, d_model // 4)  # (.., 1)\n",
    "\n",
    "    def forward(self, src):\n",
    "        # 입력 프로젝션\n",
    "        # src: (sequence_len, batch_size, feature_len)\n",
    "        x = self.input_projection(src)  # (sequence_len, batch_size, d_model)\n",
    "        x = self.pos_encoder(x)  # (sequence_len, batch_size, 4)\n",
    "\n",
    "        # 첫 번째 레이어\n",
    "        x = self.transformer_encoder1(x)  # (sequence_len, batch_size, 4)\n",
    "        x = self.projection1(x)  # (sequence_len, batch_size, 2)\n",
    "\n",
    "        # 두 번째 레이어\n",
    "        x = self.transformer_encoder2(x)  # (sequence_len, batch_size, 2)\n",
    "        x = self.projection2(x)  # (sequence_len, batch_size, 1)\n",
    "\n",
    "        return x  # style: (sequence_len, batch_size, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 디코더 모듈 정의\n",
    "class StyleBasedDecoder(nn.Module):\n",
    "    def __init__(self, output_seq_len, feature_len, num_layers=6):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.adain_layers = nn.ModuleList([AdaIN() for _ in range(num_layers)])\n",
    "        self.conv_layers = nn.ModuleList(\n",
    "            [\n",
    "                nn.Sequential(\n",
    "                    nn.Conv1d(feature_len, feature_len, kernel_size=3, stride=1, padding=1),\n",
    "                    nn.ReLU() \n",
    "                )\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.last_fpn_layer = nn.Sequential(\n",
    "            nn.Linear(output_seq_len * feature_len, output_seq_len * feature_len *4),\n",
    "            nn.ReLU(), \n",
    "            nn.Linear(output_seq_len * feature_len * 4, output_seq_len * feature_len)\n",
    "        )\n",
    "\n",
    "    def forward(self, origin_x, x, style):\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "\n",
    "            # (output_seq_len:n, batch_size, feature_len)\n",
    "            adain_x = self.adain_layers[i](x, style)\n",
    "\n",
    "            # return to (output_seq_len:n, batch_size, feature_len)\n",
    "            conv_x = self.conv_layers[i](adain_x.permute(1, 2, 0)).permute(2, 0, 1)\n",
    "\n",
    "            # add adain_x as residual # (output_seq_len:n, batch_size, feature_len)\n",
    "            x = conv_x + adain_x\n",
    "\n",
    "            # 입력 origin_x에 대한 axis 1에 대한 표준편차를 구하고 이를 기반으로 노이즈 생성\n",
    "            std = torch.std(origin_x, dim=0, keepdim=True)\n",
    "            noise = torch.randn_like(x) * std\n",
    "\n",
    "            # add noise\n",
    "            x += noise \n",
    "\n",
    "        # last FPN layer\n",
    "        output_seq_len, batch_size, feature_len = x.shape\n",
    "        x = x.permute(1, 0, 2).reshape(batch_size, -1)\n",
    "        x = self.last_fpn_layer(x)  # Apply FCN\n",
    "        x = x.reshape(batch_size, output_seq_len, feature_len).permute(1, 0, 2)\n",
    "\n",
    "        return x  # (output_seq_len:n, batch_size, feature_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 모델\n",
    "class StyleBasedTransformer(nn.Module):\n",
    "    def __init__(self, feature_len, output_seq_len):\n",
    "        super().__init__()\n",
    "        self.output_seq_len = output_seq_len\n",
    "        self.encoder = Encoder(\n",
    "            feature_len,\n",
    "        )  # layer 고정.\n",
    "        self.decoder = StyleBasedDecoder(\n",
    "            output_seq_len=self.output_seq_len, feature_len=feature_len, num_layers=6\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (seqence_len, batch_size, 1)\n",
    "        style = self.encoder(x)\n",
    "\n",
    "        # 임의 초기 시계열 값. eq.8.  # (output_seq_len:n, batch_size, feature_len)\n",
    "        initial_timeseries = calculate_linear_regression_parallel(x)[\n",
    "            : self.output_seq_len, :, :\n",
    "        ]\n",
    "\n",
    "        # (output_seq_len:n, batch_size, feature_len)\n",
    "        pred = self.decoder(origin_x=x, x=initial_timeseries, style=style)\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평가 지표 계산하는 함수 정의 - RMSE, Corr, 논문 eq.10\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    # (time_seq, feature_len)\n",
    "    rmse = np.sqrt(np.mean((y_true - y_pred) ** 2, axis=0))\n",
    "\n",
    "    y_true_mean = np.mean(y_true, axis = 0)\n",
    "    y_pred_mean = np.mean(y_pred, axis = 0)\n",
    "\n",
    "    numerator = np.sum((y_true - y_true_mean) * (y_pred - y_pred_mean), axis = 0)\n",
    "    denominator = (\n",
    "        np.sqrt(np.sum((y_true - y_true_mean) ** 2, axis=0) * np.sum((y_pred - y_pred_mean) ** 2, axis=0)) + np.finfo('float32').eps\n",
    "    )\n",
    "    corr = numerator / denominator\n",
    "\n",
    "    return rmse, corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 학습 및 평가\n",
    "def train_evaluate(hyperparams, dataset):\n",
    "    pred_length = hyperparams[\"pred_length\"]\n",
    "    batch_size = hyperparams[\"batch_size\"]\n",
    "    epochs = hyperparams[\"epochs\"]\n",
    "    learning_rate = hyperparams[\"learning_rate\"]\n",
    "\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    test_size = len(dataset) - train_size\n",
    "\n",
    "    train_set, test_set = random_split(dataset, [train_size, test_size])\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_set, batch_size=batch_size, shuffle=True, drop_last=True\n",
    "    )\n",
    "    test_loader = DataLoader(test_set, batch_size=1, drop_last=True)\n",
    "\n",
    "    # 모델 초기화\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = StyleBasedTransformer(\n",
    "        feature_len=dataset.feature_len, output_seq_len=pred_length\n",
    "    ).to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # 학습\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = []\n",
    "        for x, y in train_loader:\n",
    "            # (batch_size, sequence_len, feature_len) -> (sequence_len, batch_size, feature_len)\n",
    "            x = x.to(device).permute(1, 0, 2)\n",
    "            y = y.to(device).permute(1, 0, 2)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # (output_seq_len:n, batch_size, feature_len)\n",
    "            output = model(x)\n",
    "\n",
    "            loss = criterion(output, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss.append(loss.item())\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Train Loss: {np.mean(train_loss):.4f}\")\n",
    "\n",
    "        # 학습 중간에 예측 결과 확인을 위한 역변환\n",
    "        if (epoch + 1) % 100 == 0:\n",
    "            model.eval()\n",
    "            val_loss = []\n",
    "            with torch.no_grad():\n",
    "                for test_x, test_y in test_loader:\n",
    "                    test_x = test_x.to(device).permute(1, 0, 2)\n",
    "                    test_y = test_y.to(device).permute(1, 0, 2)\n",
    "                    test_pred = model(test_x)\n",
    "                    loss = criterion(test_pred, test_y)\n",
    "                    val_loss.append(loss.item())\n",
    "\n",
    "                # 샘플 데이터 하나 선택\n",
    "                sample_x, sample_y = next(iter(test_loader))\n",
    "                sample_x = sample_x.to(device).permute(1, 0, 2)\n",
    "                sample_y = sample_y.to(device).permute(1, 0, 2)\n",
    "\n",
    "                # 예측 (output_seq_len:n, batch_size, feature_len)\n",
    "                sample_pred = model(sample_x)\n",
    "\n",
    "                # CPU로 이동 및 numpy 변환\n",
    "                sample_y = sample_y.cpu().numpy()\n",
    "                sample_pred = sample_pred.cpu().numpy()\n",
    "\n",
    "                # 첫 번째 배치의 첫 번째 시계열만 선택\n",
    "                sample_y = sample_y[:, 0, :]  # (pred_length,)\n",
    "                sample_pred = sample_pred[:, 0, :]  # (pred_length,)\n",
    "\n",
    "                # dataset의 스케일러로 역변환\n",
    "                sample_y = dataset.scaler.inverse_transform(sample_y).flatten()\n",
    "                sample_pred = dataset.scaler.inverse_transform(sample_pred).flatten()\n",
    "\n",
    "                print(f\"\\n검증 손실 (MSE): {np.mean(val_loss):.4f}\")\n",
    "                print(f\"실제값: {sample_y[:5]}\")\n",
    "                print(f\"예측값: {sample_pred[:5]}\\n\")\n",
    "\n",
    "    # 모델 평가\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        list_test_y = []\n",
    "        list_test_pred = []\n",
    "        for test_x, test_y in test_loader:\n",
    "            test_x = test_x.to(device).permute(1, 0, 2)\n",
    "            test_y = test_y.to(device).permute(1, 0, 2)\n",
    "            test_pred = model(test_x)\n",
    "\n",
    "            list_test_y.append(test_y)\n",
    "            list_test_pred.append(test_pred)\n",
    "\n",
    "        # 마지막 1000개 데이터에 대한 결과.\n",
    "        total_test_y = torch.cat(list_test_y, dim=0)[-1000:, ...]\n",
    "        total_pred_y = torch.cat(list_test_pred, dim=0)[-1000:, ...]\n",
    "\n",
    "        total_test_y = total_test_y.detach().cpu().numpy().reshape(1000, -1)\n",
    "        total_pred_y = total_pred_y.detach().cpu().numpy().reshape(1000, -1)\n",
    "\n",
    "    return calculate_metrics(total_test_y, total_pred_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하이퍼파라미터 설정\n",
    "hyperparams = {\n",
    "    \"sequence_len\": 30,\n",
    "    \"sliding_window\": 2,\n",
    "    \"pred_length\": 10,\n",
    "    \"batch_size\": 1024,\n",
    "    \"epochs\": 300,\n",
    "    \"learning_rate\": 0.001,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0번째 변수에 대한 학습 및 테스트 시작\n",
      "Epoch [1/300], Train Loss: 0.0742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/emotion-recognition/lib/python3.10/site-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/300], Train Loss: 0.0623\n",
      "Epoch [3/300], Train Loss: 0.0513\n",
      "Epoch [4/300], Train Loss: 0.0412\n",
      "Epoch [5/300], Train Loss: 0.0351\n",
      "Epoch [6/300], Train Loss: 0.0325\n",
      "Epoch [7/300], Train Loss: 0.0319\n",
      "Epoch [8/300], Train Loss: 0.0315\n",
      "Epoch [9/300], Train Loss: 0.0310\n",
      "Epoch [10/300], Train Loss: 0.0309\n",
      "Epoch [11/300], Train Loss: 0.0307\n",
      "Epoch [12/300], Train Loss: 0.0304\n",
      "Epoch [13/300], Train Loss: 0.0302\n",
      "Epoch [14/300], Train Loss: 0.0298\n",
      "Epoch [15/300], Train Loss: 0.0297\n",
      "Epoch [16/300], Train Loss: 0.0295\n",
      "Epoch [17/300], Train Loss: 0.0292\n",
      "Epoch [18/300], Train Loss: 0.0292\n",
      "Epoch [19/300], Train Loss: 0.0290\n",
      "Epoch [20/300], Train Loss: 0.0287\n",
      "Epoch [21/300], Train Loss: 0.0284\n",
      "Epoch [22/300], Train Loss: 0.0285\n",
      "Epoch [23/300], Train Loss: 0.0283\n",
      "Epoch [24/300], Train Loss: 0.0282\n",
      "Epoch [25/300], Train Loss: 0.0280\n",
      "Epoch [26/300], Train Loss: 0.0279\n",
      "Epoch [27/300], Train Loss: 0.0276\n",
      "Epoch [28/300], Train Loss: 0.0276\n",
      "Epoch [29/300], Train Loss: 0.0274\n",
      "Epoch [30/300], Train Loss: 0.0275\n",
      "Epoch [31/300], Train Loss: 0.0272\n",
      "Epoch [32/300], Train Loss: 0.0272\n",
      "Epoch [33/300], Train Loss: 0.0271\n",
      "Epoch [34/300], Train Loss: 0.0271\n",
      "Epoch [35/300], Train Loss: 0.0271\n",
      "Epoch [36/300], Train Loss: 0.0269\n",
      "Epoch [37/300], Train Loss: 0.0266\n",
      "Epoch [38/300], Train Loss: 0.0267\n",
      "Epoch [39/300], Train Loss: 0.0268\n",
      "Epoch [40/300], Train Loss: 0.0268\n",
      "Epoch [41/300], Train Loss: 0.0267\n",
      "Epoch [42/300], Train Loss: 0.0268\n",
      "Epoch [43/300], Train Loss: 0.0265\n",
      "Epoch [44/300], Train Loss: 0.0265\n",
      "Epoch [45/300], Train Loss: 0.0265\n",
      "Epoch [46/300], Train Loss: 0.0264\n",
      "Epoch [47/300], Train Loss: 0.0263\n",
      "Epoch [48/300], Train Loss: 0.0264\n",
      "Epoch [49/300], Train Loss: 0.0264\n",
      "Epoch [50/300], Train Loss: 0.0263\n",
      "Epoch [51/300], Train Loss: 0.0263\n",
      "Epoch [52/300], Train Loss: 0.0264\n",
      "Epoch [53/300], Train Loss: 0.0263\n",
      "Epoch [54/300], Train Loss: 0.0264\n",
      "Epoch [55/300], Train Loss: 0.0262\n",
      "Epoch [56/300], Train Loss: 0.0261\n",
      "Epoch [57/300], Train Loss: 0.0260\n",
      "Epoch [58/300], Train Loss: 0.0262\n",
      "Epoch [59/300], Train Loss: 0.0261\n",
      "Epoch [60/300], Train Loss: 0.0262\n",
      "Epoch [61/300], Train Loss: 0.0260\n",
      "Epoch [62/300], Train Loss: 0.0259\n",
      "Epoch [63/300], Train Loss: 0.0262\n",
      "Epoch [64/300], Train Loss: 0.0262\n",
      "Epoch [65/300], Train Loss: 0.0260\n",
      "Epoch [66/300], Train Loss: 0.0261\n",
      "Epoch [67/300], Train Loss: 0.0260\n",
      "Epoch [68/300], Train Loss: 0.0263\n",
      "Epoch [69/300], Train Loss: 0.0259\n",
      "Epoch [70/300], Train Loss: 0.0261\n",
      "Epoch [71/300], Train Loss: 0.0259\n",
      "Epoch [72/300], Train Loss: 0.0261\n",
      "Epoch [73/300], Train Loss: 0.0259\n",
      "Epoch [74/300], Train Loss: 0.0259\n",
      "Epoch [75/300], Train Loss: 0.0260\n",
      "Epoch [76/300], Train Loss: 0.0260\n",
      "Epoch [77/300], Train Loss: 0.0262\n",
      "Epoch [78/300], Train Loss: 0.0262\n",
      "Epoch [79/300], Train Loss: 0.0260\n",
      "Epoch [80/300], Train Loss: 0.0258\n",
      "Epoch [81/300], Train Loss: 0.0260\n",
      "Epoch [82/300], Train Loss: 0.0262\n",
      "Epoch [83/300], Train Loss: 0.0261\n",
      "Epoch [84/300], Train Loss: 0.0258\n",
      "Epoch [85/300], Train Loss: 0.0259\n",
      "Epoch [86/300], Train Loss: 0.0262\n",
      "Epoch [87/300], Train Loss: 0.0258\n",
      "Epoch [88/300], Train Loss: 0.0261\n",
      "Epoch [89/300], Train Loss: 0.0259\n",
      "Epoch [90/300], Train Loss: 0.0258\n",
      "Epoch [91/300], Train Loss: 0.0259\n",
      "Epoch [92/300], Train Loss: 0.0260\n",
      "Epoch [93/300], Train Loss: 0.0257\n",
      "Epoch [94/300], Train Loss: 0.0260\n",
      "Epoch [95/300], Train Loss: 0.0258\n",
      "Epoch [96/300], Train Loss: 0.0258\n",
      "Epoch [97/300], Train Loss: 0.0258\n",
      "Epoch [98/300], Train Loss: 0.0258\n",
      "Epoch [99/300], Train Loss: 0.0260\n",
      "Epoch [100/300], Train Loss: 0.0260\n",
      "\n",
      "검증 손실 (MSE): 0.0265\n",
      "실제값: [67.25888  73.60406  58.375633 73.60406  59.64467 ]\n",
      "예측값: [44.634163 45.857414 46.51582  45.52574  46.88732 ]\n",
      "\n",
      "Epoch [101/300], Train Loss: 0.0259\n",
      "Epoch [102/300], Train Loss: 0.0256\n",
      "Epoch [103/300], Train Loss: 0.0260\n",
      "Epoch [104/300], Train Loss: 0.0259\n",
      "Epoch [105/300], Train Loss: 0.0259\n",
      "Epoch [106/300], Train Loss: 0.0258\n",
      "Epoch [107/300], Train Loss: 0.0258\n",
      "Epoch [108/300], Train Loss: 0.0258\n",
      "Epoch [109/300], Train Loss: 0.0259\n",
      "Epoch [110/300], Train Loss: 0.0258\n",
      "Epoch [111/300], Train Loss: 0.0256\n",
      "Epoch [112/300], Train Loss: 0.0256\n",
      "Epoch [113/300], Train Loss: 0.0259\n",
      "Epoch [114/300], Train Loss: 0.0258\n",
      "Epoch [115/300], Train Loss: 0.0258\n",
      "Epoch [116/300], Train Loss: 0.0259\n",
      "Epoch [117/300], Train Loss: 0.0257\n",
      "Epoch [118/300], Train Loss: 0.0256\n",
      "Epoch [119/300], Train Loss: 0.0257\n",
      "Epoch [120/300], Train Loss: 0.0257\n",
      "Epoch [121/300], Train Loss: 0.0257\n",
      "Epoch [122/300], Train Loss: 0.0258\n",
      "Epoch [123/300], Train Loss: 0.0259\n",
      "Epoch [124/300], Train Loss: 0.0257\n",
      "Epoch [125/300], Train Loss: 0.0257\n",
      "Epoch [126/300], Train Loss: 0.0258\n",
      "Epoch [127/300], Train Loss: 0.0256\n",
      "Epoch [128/300], Train Loss: 0.0256\n",
      "Epoch [129/300], Train Loss: 0.0257\n",
      "Epoch [130/300], Train Loss: 0.0257\n",
      "Epoch [131/300], Train Loss: 0.0256\n",
      "Epoch [132/300], Train Loss: 0.0256\n",
      "Epoch [133/300], Train Loss: 0.0257\n",
      "Epoch [134/300], Train Loss: 0.0259\n",
      "Epoch [135/300], Train Loss: 0.0256\n",
      "Epoch [136/300], Train Loss: 0.0257\n",
      "Epoch [137/300], Train Loss: 0.0257\n",
      "Epoch [138/300], Train Loss: 0.0258\n",
      "Epoch [139/300], Train Loss: 0.0256\n",
      "Epoch [140/300], Train Loss: 0.0254\n",
      "Epoch [141/300], Train Loss: 0.0256\n",
      "Epoch [142/300], Train Loss: 0.0255\n",
      "Epoch [143/300], Train Loss: 0.0255\n",
      "Epoch [144/300], Train Loss: 0.0256\n",
      "Epoch [145/300], Train Loss: 0.0256\n",
      "Epoch [146/300], Train Loss: 0.0256\n",
      "Epoch [147/300], Train Loss: 0.0257\n",
      "Epoch [148/300], Train Loss: 0.0255\n",
      "Epoch [149/300], Train Loss: 0.0254\n",
      "Epoch [150/300], Train Loss: 0.0254\n",
      "Epoch [151/300], Train Loss: 0.0255\n",
      "Epoch [152/300], Train Loss: 0.0258\n",
      "Epoch [153/300], Train Loss: 0.0255\n",
      "Epoch [154/300], Train Loss: 0.0256\n",
      "Epoch [155/300], Train Loss: 0.0255\n",
      "Epoch [156/300], Train Loss: 0.0257\n",
      "Epoch [157/300], Train Loss: 0.0256\n",
      "Epoch [158/300], Train Loss: 0.0255\n",
      "Epoch [159/300], Train Loss: 0.0256\n",
      "Epoch [160/300], Train Loss: 0.0254\n",
      "Epoch [161/300], Train Loss: 0.0256\n",
      "Epoch [162/300], Train Loss: 0.0256\n",
      "Epoch [163/300], Train Loss: 0.0254\n",
      "Epoch [164/300], Train Loss: 0.0255\n",
      "Epoch [165/300], Train Loss: 0.0256\n",
      "Epoch [166/300], Train Loss: 0.0254\n",
      "Epoch [167/300], Train Loss: 0.0256\n",
      "Epoch [168/300], Train Loss: 0.0252\n",
      "Epoch [169/300], Train Loss: 0.0254\n",
      "Epoch [170/300], Train Loss: 0.0256\n",
      "Epoch [171/300], Train Loss: 0.0255\n",
      "Epoch [172/300], Train Loss: 0.0255\n",
      "Epoch [173/300], Train Loss: 0.0255\n",
      "Epoch [174/300], Train Loss: 0.0254\n",
      "Epoch [175/300], Train Loss: 0.0254\n",
      "Epoch [176/300], Train Loss: 0.0255\n",
      "Epoch [177/300], Train Loss: 0.0254\n",
      "Epoch [178/300], Train Loss: 0.0254\n",
      "Epoch [179/300], Train Loss: 0.0254\n",
      "Epoch [180/300], Train Loss: 0.0255\n",
      "Epoch [181/300], Train Loss: 0.0254\n",
      "Epoch [182/300], Train Loss: 0.0255\n",
      "Epoch [183/300], Train Loss: 0.0255\n",
      "Epoch [184/300], Train Loss: 0.0252\n",
      "Epoch [185/300], Train Loss: 0.0255\n",
      "Epoch [186/300], Train Loss: 0.0253\n",
      "Epoch [187/300], Train Loss: 0.0254\n",
      "Epoch [188/300], Train Loss: 0.0255\n",
      "Epoch [189/300], Train Loss: 0.0253\n",
      "Epoch [190/300], Train Loss: 0.0256\n",
      "Epoch [191/300], Train Loss: 0.0254\n",
      "Epoch [192/300], Train Loss: 0.0254\n",
      "Epoch [193/300], Train Loss: 0.0254\n",
      "Epoch [194/300], Train Loss: 0.0254\n",
      "Epoch [195/300], Train Loss: 0.0255\n",
      "Epoch [196/300], Train Loss: 0.0253\n",
      "Epoch [197/300], Train Loss: 0.0254\n",
      "Epoch [198/300], Train Loss: 0.0253\n",
      "Epoch [199/300], Train Loss: 0.0254\n",
      "Epoch [200/300], Train Loss: 0.0253\n",
      "\n",
      "검증 손실 (MSE): 0.0258\n",
      "실제값: [67.25888  73.60406  58.375633 73.60406  59.64467 ]\n",
      "예측값: [43.64926  42.717842 41.64032  39.75667  44.245487]\n",
      "\n",
      "Epoch [201/300], Train Loss: 0.0255\n",
      "Epoch [202/300], Train Loss: 0.0253\n",
      "Epoch [203/300], Train Loss: 0.0255\n",
      "Epoch [204/300], Train Loss: 0.0255\n",
      "Epoch [205/300], Train Loss: 0.0254\n",
      "Epoch [206/300], Train Loss: 0.0253\n",
      "Epoch [207/300], Train Loss: 0.0255\n",
      "Epoch [208/300], Train Loss: 0.0252\n",
      "Epoch [209/300], Train Loss: 0.0252\n",
      "Epoch [210/300], Train Loss: 0.0254\n",
      "Epoch [211/300], Train Loss: 0.0251\n",
      "Epoch [212/300], Train Loss: 0.0252\n",
      "Epoch [213/300], Train Loss: 0.0250\n",
      "Epoch [214/300], Train Loss: 0.0253\n",
      "Epoch [215/300], Train Loss: 0.0256\n",
      "Epoch [216/300], Train Loss: 0.0253\n",
      "Epoch [217/300], Train Loss: 0.0253\n",
      "Epoch [218/300], Train Loss: 0.0252\n",
      "Epoch [219/300], Train Loss: 0.0254\n",
      "Epoch [220/300], Train Loss: 0.0254\n",
      "Epoch [221/300], Train Loss: 0.0253\n",
      "Epoch [222/300], Train Loss: 0.0254\n",
      "Epoch [223/300], Train Loss: 0.0252\n",
      "Epoch [224/300], Train Loss: 0.0253\n",
      "Epoch [225/300], Train Loss: 0.0252\n",
      "Epoch [226/300], Train Loss: 0.0251\n",
      "Epoch [227/300], Train Loss: 0.0253\n",
      "Epoch [228/300], Train Loss: 0.0254\n",
      "Epoch [229/300], Train Loss: 0.0253\n",
      "Epoch [230/300], Train Loss: 0.0253\n",
      "Epoch [231/300], Train Loss: 0.0250\n",
      "Epoch [232/300], Train Loss: 0.0252\n",
      "Epoch [233/300], Train Loss: 0.0253\n",
      "Epoch [234/300], Train Loss: 0.0253\n",
      "Epoch [235/300], Train Loss: 0.0251\n",
      "Epoch [236/300], Train Loss: 0.0254\n",
      "Epoch [237/300], Train Loss: 0.0252\n",
      "Epoch [238/300], Train Loss: 0.0253\n",
      "Epoch [239/300], Train Loss: 0.0252\n",
      "Epoch [240/300], Train Loss: 0.0254\n",
      "Epoch [241/300], Train Loss: 0.0251\n",
      "Epoch [242/300], Train Loss: 0.0250\n",
      "Epoch [243/300], Train Loss: 0.0253\n",
      "Epoch [244/300], Train Loss: 0.0253\n",
      "Epoch [245/300], Train Loss: 0.0251\n",
      "Epoch [246/300], Train Loss: 0.0251\n",
      "Epoch [247/300], Train Loss: 0.0252\n",
      "Epoch [248/300], Train Loss: 0.0252\n",
      "Epoch [249/300], Train Loss: 0.0252\n",
      "Epoch [250/300], Train Loss: 0.0254\n",
      "Epoch [251/300], Train Loss: 0.0252\n",
      "Epoch [252/300], Train Loss: 0.0251\n",
      "Epoch [253/300], Train Loss: 0.0251\n",
      "Epoch [254/300], Train Loss: 0.0253\n",
      "Epoch [255/300], Train Loss: 0.0253\n",
      "Epoch [256/300], Train Loss: 0.0254\n",
      "Epoch [257/300], Train Loss: 0.0254\n",
      "Epoch [258/300], Train Loss: 0.0252\n",
      "Epoch [259/300], Train Loss: 0.0253\n",
      "Epoch [260/300], Train Loss: 0.0253\n",
      "Epoch [261/300], Train Loss: 0.0252\n",
      "Epoch [262/300], Train Loss: 0.0251\n",
      "Epoch [263/300], Train Loss: 0.0251\n",
      "Epoch [264/300], Train Loss: 0.0252\n",
      "Epoch [265/300], Train Loss: 0.0252\n",
      "Epoch [266/300], Train Loss: 0.0253\n",
      "Epoch [267/300], Train Loss: 0.0251\n",
      "Epoch [268/300], Train Loss: 0.0253\n",
      "Epoch [269/300], Train Loss: 0.0253\n",
      "Epoch [270/300], Train Loss: 0.0253\n",
      "Epoch [271/300], Train Loss: 0.0250\n",
      "Epoch [272/300], Train Loss: 0.0249\n",
      "Epoch [273/300], Train Loss: 0.0248\n",
      "Epoch [274/300], Train Loss: 0.0255\n",
      "Epoch [275/300], Train Loss: 0.0254\n",
      "Epoch [276/300], Train Loss: 0.0251\n",
      "Epoch [277/300], Train Loss: 0.0252\n",
      "Epoch [278/300], Train Loss: 0.0251\n",
      "Epoch [279/300], Train Loss: 0.0251\n",
      "Epoch [280/300], Train Loss: 0.0251\n",
      "Epoch [281/300], Train Loss: 0.0250\n",
      "Epoch [282/300], Train Loss: 0.0252\n",
      "Epoch [283/300], Train Loss: 0.0251\n",
      "Epoch [284/300], Train Loss: 0.0251\n",
      "Epoch [285/300], Train Loss: 0.0252\n",
      "Epoch [286/300], Train Loss: 0.0250\n",
      "Epoch [287/300], Train Loss: 0.0251\n",
      "Epoch [288/300], Train Loss: 0.0253\n",
      "Epoch [289/300], Train Loss: 0.0250\n",
      "Epoch [290/300], Train Loss: 0.0251\n",
      "Epoch [291/300], Train Loss: 0.0249\n",
      "Epoch [292/300], Train Loss: 0.0250\n",
      "Epoch [293/300], Train Loss: 0.0248\n",
      "Epoch [294/300], Train Loss: 0.0251\n",
      "Epoch [295/300], Train Loss: 0.0250\n",
      "Epoch [296/300], Train Loss: 0.0252\n",
      "Epoch [297/300], Train Loss: 0.0251\n",
      "Epoch [298/300], Train Loss: 0.0251\n",
      "Epoch [299/300], Train Loss: 0.0252\n",
      "Epoch [300/300], Train Loss: 0.0249\n",
      "\n",
      "검증 손실 (MSE): 0.0256\n",
      "실제값: [67.25888  73.60406  58.375633 73.60406  59.64467 ]\n",
      "예측값: [39.30304  38.832157 38.00915  39.20946  37.146404]\n",
      "\n",
      "0번째 변수 학습 및 테스트 종료\n",
      "\n",
      "==================================================\n",
      "[단변수(var=0) 학습 모델 평가 결과]\n",
      "소요된 시간: 75.13s, 모든 변수에 대한 학습시 예상 시간: 27797.49s\n",
      "RMSE: 0.1487\n",
      "Corr: 0.4188\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# 실험 1) 논문의 실험 방식과 동일하게 진행 - 단일 client 입력으로 학습 및 평가\n",
    "start_time = time.time()\n",
    "client_num = 0 # 논문 실험 결과는 1번째 변수에 대한 마지막 1000개 데이터에 대한 출력 값\n",
    "dataset = TimeSeriesDataset(\"./LD2011_2014.txt\", hyperparams[\"sequence_len\"], hyperparams[\"pred_length\"], hyperparams[\"sliding_window\"], client = client_num)\n",
    "print(f\"{client_num}번째 변수에 대한 학습 및 테스트 시작\")\n",
    "rmse, corr = train_evaluate(hyperparams, dataset)\n",
    "print(f\"{client_num}번째 변수 학습 및 테스트 종료\\n\")\n",
    "end_time = time.time()\n",
    "elapsed_time_one_client = end_time - start_time\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(f\"[단변수(var={client_num}) 학습 모델 평가 결과]\")\n",
    "print(f\"소요된 시간: {elapsed_time_one_client:.2f}s, 모든 변수에 대한 학습시 예상 시간: {elapsed_time_one_client * 370:.2f}s\")\n",
    "print(f\"RMSE: {rmse.squeeze():.4f}\")\n",
    "print(f\"Corr: {corr.squeeze():.4f}\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "다변수 학습 및 테스트 시작\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/emotion-recognition/lib/python3.10/site-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/300], Train Loss: 2.4599\n",
      "Epoch [2/300], Train Loss: 0.1886\n",
      "Epoch [3/300], Train Loss: 0.0549\n",
      "Epoch [4/300], Train Loss: 0.0378\n",
      "Epoch [5/300], Train Loss: 0.0317\n",
      "Epoch [6/300], Train Loss: 0.0298\n",
      "Epoch [7/300], Train Loss: 0.0286\n",
      "Epoch [8/300], Train Loss: 0.0272\n",
      "Epoch [9/300], Train Loss: 0.0251\n",
      "Epoch [10/300], Train Loss: 0.0230\n",
      "Epoch [11/300], Train Loss: 0.0217\n",
      "Epoch [12/300], Train Loss: 0.0206\n",
      "Epoch [13/300], Train Loss: 0.0197\n",
      "Epoch [14/300], Train Loss: 0.0188\n",
      "Epoch [15/300], Train Loss: 0.0180\n",
      "Epoch [16/300], Train Loss: 0.0172\n",
      "Epoch [17/300], Train Loss: 0.0166\n",
      "Epoch [18/300], Train Loss: 0.0159\n",
      "Epoch [19/300], Train Loss: 0.0154\n",
      "Epoch [20/300], Train Loss: 0.0149\n",
      "Epoch [21/300], Train Loss: 0.0146\n",
      "Epoch [22/300], Train Loss: 0.0141\n",
      "Epoch [23/300], Train Loss: 0.0138\n",
      "Epoch [24/300], Train Loss: 0.0136\n",
      "Epoch [25/300], Train Loss: 0.0134\n",
      "Epoch [26/300], Train Loss: 0.0133\n",
      "Epoch [27/300], Train Loss: 0.0130\n",
      "Epoch [28/300], Train Loss: 0.0128\n",
      "Epoch [29/300], Train Loss: 0.0126\n",
      "Epoch [30/300], Train Loss: 0.0125\n",
      "Epoch [31/300], Train Loss: 0.0126\n",
      "Epoch [32/300], Train Loss: 0.0125\n",
      "Epoch [33/300], Train Loss: 0.0120\n",
      "Epoch [34/300], Train Loss: 0.0118\n",
      "Epoch [35/300], Train Loss: 0.0117\n",
      "Epoch [36/300], Train Loss: 0.0116\n",
      "Epoch [37/300], Train Loss: 0.0113\n",
      "Epoch [38/300], Train Loss: 0.0112\n",
      "Epoch [39/300], Train Loss: 0.0111\n",
      "Epoch [40/300], Train Loss: 0.0110\n",
      "Epoch [41/300], Train Loss: 0.0109\n",
      "Epoch [42/300], Train Loss: 0.0109\n",
      "Epoch [43/300], Train Loss: 0.0109\n",
      "Epoch [44/300], Train Loss: 0.0109\n",
      "Epoch [45/300], Train Loss: 0.0106\n",
      "Epoch [46/300], Train Loss: 0.0103\n",
      "Epoch [47/300], Train Loss: 0.0102\n",
      "Epoch [48/300], Train Loss: 0.0101\n",
      "Epoch [49/300], Train Loss: 0.0100\n",
      "Epoch [50/300], Train Loss: 0.0100\n",
      "Epoch [51/300], Train Loss: 0.0100\n",
      "Epoch [52/300], Train Loss: 0.0099\n",
      "Epoch [53/300], Train Loss: 0.0099\n",
      "Epoch [54/300], Train Loss: 0.0099\n",
      "Epoch [55/300], Train Loss: 0.0096\n",
      "Epoch [56/300], Train Loss: 0.0094\n",
      "Epoch [57/300], Train Loss: 0.0094\n",
      "Epoch [58/300], Train Loss: 0.0093\n",
      "Epoch [59/300], Train Loss: 0.0092\n",
      "Epoch [60/300], Train Loss: 0.0091\n",
      "Epoch [61/300], Train Loss: 0.0091\n",
      "Epoch [62/300], Train Loss: 0.0090\n",
      "Epoch [63/300], Train Loss: 0.0091\n",
      "Epoch [64/300], Train Loss: 0.0089\n",
      "Epoch [65/300], Train Loss: 0.0088\n",
      "Epoch [66/300], Train Loss: 0.0089\n",
      "Epoch [67/300], Train Loss: 0.0087\n",
      "Epoch [68/300], Train Loss: 0.0086\n",
      "Epoch [69/300], Train Loss: 0.0085\n",
      "Epoch [70/300], Train Loss: 0.0084\n",
      "Epoch [71/300], Train Loss: 0.0084\n",
      "Epoch [72/300], Train Loss: 0.0084\n",
      "Epoch [73/300], Train Loss: 0.0084\n",
      "Epoch [74/300], Train Loss: 0.0083\n",
      "Epoch [75/300], Train Loss: 0.0082\n",
      "Epoch [76/300], Train Loss: 0.0081\n",
      "Epoch [77/300], Train Loss: 0.0081\n",
      "Epoch [78/300], Train Loss: 0.0081\n",
      "Epoch [79/300], Train Loss: 0.0080\n",
      "Epoch [80/300], Train Loss: 0.0079\n",
      "Epoch [81/300], Train Loss: 0.0079\n",
      "Epoch [82/300], Train Loss: 0.0078\n",
      "Epoch [83/300], Train Loss: 0.0078\n",
      "Epoch [84/300], Train Loss: 0.0077\n",
      "Epoch [85/300], Train Loss: 0.0077\n",
      "Epoch [86/300], Train Loss: 0.0077\n",
      "Epoch [87/300], Train Loss: 0.0076\n",
      "Epoch [88/300], Train Loss: 0.0076\n",
      "Epoch [89/300], Train Loss: 0.0075\n",
      "Epoch [90/300], Train Loss: 0.0075\n",
      "Epoch [91/300], Train Loss: 0.0075\n",
      "Epoch [92/300], Train Loss: 0.0073\n",
      "Epoch [93/300], Train Loss: 0.0073\n",
      "Epoch [94/300], Train Loss: 0.0073\n",
      "Epoch [95/300], Train Loss: 0.0073\n",
      "Epoch [96/300], Train Loss: 0.0073\n",
      "Epoch [97/300], Train Loss: 0.0073\n",
      "Epoch [98/300], Train Loss: 0.0072\n",
      "Epoch [99/300], Train Loss: 0.0071\n",
      "Epoch [100/300], Train Loss: 0.0071\n",
      "\n",
      "검증 손실 (MSE): 0.0115\n",
      "실제값: [ 67.25888  123.75534    6.950478 817.0732   347.56097 ]\n",
      "예측값: [ 19.529924 129.55362   20.301708 768.0351   278.46185 ]\n",
      "\n",
      "Epoch [101/300], Train Loss: 0.0071\n",
      "Epoch [102/300], Train Loss: 0.0070\n",
      "Epoch [103/300], Train Loss: 0.0070\n",
      "Epoch [104/300], Train Loss: 0.0069\n",
      "Epoch [105/300], Train Loss: 0.0068\n",
      "Epoch [106/300], Train Loss: 0.0069\n",
      "Epoch [107/300], Train Loss: 0.0068\n",
      "Epoch [108/300], Train Loss: 0.0068\n",
      "Epoch [109/300], Train Loss: 0.0067\n",
      "Epoch [110/300], Train Loss: 0.0067\n",
      "Epoch [111/300], Train Loss: 0.0066\n",
      "Epoch [112/300], Train Loss: 0.0066\n",
      "Epoch [113/300], Train Loss: 0.0066\n",
      "Epoch [114/300], Train Loss: 0.0065\n",
      "Epoch [115/300], Train Loss: 0.0066\n",
      "Epoch [116/300], Train Loss: 0.0065\n",
      "Epoch [117/300], Train Loss: 0.0065\n",
      "Epoch [118/300], Train Loss: 0.0065\n",
      "Epoch [119/300], Train Loss: 0.0065\n",
      "Epoch [120/300], Train Loss: 0.0065\n",
      "Epoch [121/300], Train Loss: 0.0065\n",
      "Epoch [122/300], Train Loss: 0.0064\n",
      "Epoch [123/300], Train Loss: 0.0063\n",
      "Epoch [124/300], Train Loss: 0.0063\n",
      "Epoch [125/300], Train Loss: 0.0063\n",
      "Epoch [126/300], Train Loss: 0.0062\n",
      "Epoch [127/300], Train Loss: 0.0063\n",
      "Epoch [128/300], Train Loss: 0.0062\n",
      "Epoch [129/300], Train Loss: 0.0061\n",
      "Epoch [130/300], Train Loss: 0.0062\n",
      "Epoch [131/300], Train Loss: 0.0062\n",
      "Epoch [132/300], Train Loss: 0.0061\n",
      "Epoch [133/300], Train Loss: 0.0061\n",
      "Epoch [134/300], Train Loss: 0.0061\n",
      "Epoch [135/300], Train Loss: 0.0060\n",
      "Epoch [136/300], Train Loss: 0.0060\n",
      "Epoch [137/300], Train Loss: 0.0060\n",
      "Epoch [138/300], Train Loss: 0.0059\n",
      "Epoch [139/300], Train Loss: 0.0059\n",
      "Epoch [140/300], Train Loss: 0.0059\n",
      "Epoch [141/300], Train Loss: 0.0059\n",
      "Epoch [142/300], Train Loss: 0.0059\n",
      "Epoch [143/300], Train Loss: 0.0058\n",
      "Epoch [144/300], Train Loss: 0.0060\n",
      "Epoch [145/300], Train Loss: 0.0059\n",
      "Epoch [146/300], Train Loss: 0.0059\n",
      "Epoch [147/300], Train Loss: 0.0058\n",
      "Epoch [148/300], Train Loss: 0.0058\n",
      "Epoch [149/300], Train Loss: 0.0058\n",
      "Epoch [150/300], Train Loss: 0.0057\n",
      "Epoch [151/300], Train Loss: 0.0057\n",
      "Epoch [152/300], Train Loss: 0.0057\n",
      "Epoch [153/300], Train Loss: 0.0056\n",
      "Epoch [154/300], Train Loss: 0.0056\n",
      "Epoch [155/300], Train Loss: 0.0056\n",
      "Epoch [156/300], Train Loss: 0.0056\n",
      "Epoch [157/300], Train Loss: 0.0056\n",
      "Epoch [158/300], Train Loss: 0.0056\n",
      "Epoch [159/300], Train Loss: 0.0055\n",
      "Epoch [160/300], Train Loss: 0.0055\n",
      "Epoch [161/300], Train Loss: 0.0056\n",
      "Epoch [162/300], Train Loss: 0.0055\n",
      "Epoch [163/300], Train Loss: 0.0055\n",
      "Epoch [164/300], Train Loss: 0.0055\n",
      "Epoch [165/300], Train Loss: 0.0055\n",
      "Epoch [166/300], Train Loss: 0.0055\n",
      "Epoch [167/300], Train Loss: 0.0055\n",
      "Epoch [168/300], Train Loss: 0.0056\n",
      "Epoch [169/300], Train Loss: 0.0055\n",
      "Epoch [170/300], Train Loss: 0.0055\n",
      "Epoch [171/300], Train Loss: 0.0054\n",
      "Epoch [172/300], Train Loss: 0.0054\n",
      "Epoch [173/300], Train Loss: 0.0054\n",
      "Epoch [174/300], Train Loss: 0.0054\n",
      "Epoch [175/300], Train Loss: 0.0054\n",
      "Epoch [176/300], Train Loss: 0.0053\n",
      "Epoch [177/300], Train Loss: 0.0053\n",
      "Epoch [178/300], Train Loss: 0.0053\n",
      "Epoch [179/300], Train Loss: 0.0053\n",
      "Epoch [180/300], Train Loss: 0.0053\n",
      "Epoch [181/300], Train Loss: 0.0054\n",
      "Epoch [182/300], Train Loss: 0.0053\n",
      "Epoch [183/300], Train Loss: 0.0052\n",
      "Epoch [184/300], Train Loss: 0.0052\n",
      "Epoch [185/300], Train Loss: 0.0053\n",
      "Epoch [186/300], Train Loss: 0.0052\n",
      "Epoch [187/300], Train Loss: 0.0052\n",
      "Epoch [188/300], Train Loss: 0.0052\n",
      "Epoch [189/300], Train Loss: 0.0051\n",
      "Epoch [190/300], Train Loss: 0.0051\n",
      "Epoch [191/300], Train Loss: 0.0051\n",
      "Epoch [192/300], Train Loss: 0.0051\n",
      "Epoch [193/300], Train Loss: 0.0051\n",
      "Epoch [194/300], Train Loss: 0.0051\n",
      "Epoch [195/300], Train Loss: 0.0051\n",
      "Epoch [196/300], Train Loss: 0.0050\n",
      "Epoch [197/300], Train Loss: 0.0050\n",
      "Epoch [198/300], Train Loss: 0.0050\n",
      "Epoch [199/300], Train Loss: 0.0050\n",
      "Epoch [200/300], Train Loss: 0.0050\n",
      "\n",
      "검증 손실 (MSE): 0.0121\n",
      "실제값: [ 67.25888  123.75534    6.950478 817.0732   347.56097 ]\n",
      "예측값: [ 39.610115 133.64386   29.580284 829.2542   333.5228  ]\n",
      "\n",
      "Epoch [201/300], Train Loss: 0.0050\n",
      "Epoch [202/300], Train Loss: 0.0050\n",
      "Epoch [203/300], Train Loss: 0.0049\n",
      "Epoch [204/300], Train Loss: 0.0049\n",
      "Epoch [205/300], Train Loss: 0.0049\n",
      "Epoch [206/300], Train Loss: 0.0049\n",
      "Epoch [207/300], Train Loss: 0.0049\n",
      "Epoch [208/300], Train Loss: 0.0048\n",
      "Epoch [209/300], Train Loss: 0.0048\n",
      "Epoch [210/300], Train Loss: 0.0050\n",
      "Epoch [211/300], Train Loss: 0.0051\n",
      "Epoch [212/300], Train Loss: 0.0050\n",
      "Epoch [213/300], Train Loss: 0.0049\n",
      "Epoch [214/300], Train Loss: 0.0049\n",
      "Epoch [215/300], Train Loss: 0.0048\n",
      "Epoch [216/300], Train Loss: 0.0049\n",
      "Epoch [217/300], Train Loss: 0.0048\n",
      "Epoch [218/300], Train Loss: 0.0048\n",
      "Epoch [219/300], Train Loss: 0.0048\n",
      "Epoch [220/300], Train Loss: 0.0048\n",
      "Epoch [221/300], Train Loss: 0.0047\n",
      "Epoch [222/300], Train Loss: 0.0047\n",
      "Epoch [223/300], Train Loss: 0.0047\n",
      "Epoch [224/300], Train Loss: 0.0047\n",
      "Epoch [225/300], Train Loss: 0.0048\n",
      "Epoch [226/300], Train Loss: 0.0048\n",
      "Epoch [227/300], Train Loss: 0.0048\n",
      "Epoch [228/300], Train Loss: 0.0047\n",
      "Epoch [229/300], Train Loss: 0.0047\n",
      "Epoch [230/300], Train Loss: 0.0047\n",
      "Epoch [231/300], Train Loss: 0.0047\n",
      "Epoch [232/300], Train Loss: 0.0047\n",
      "Epoch [233/300], Train Loss: 0.0047\n",
      "Epoch [234/300], Train Loss: 0.0046\n",
      "Epoch [235/300], Train Loss: 0.0046\n",
      "Epoch [236/300], Train Loss: 0.0046\n",
      "Epoch [237/300], Train Loss: 0.0046\n",
      "Epoch [238/300], Train Loss: 0.0046\n",
      "Epoch [239/300], Train Loss: 0.0046\n",
      "Epoch [240/300], Train Loss: 0.0045\n",
      "Epoch [241/300], Train Loss: 0.0045\n",
      "Epoch [242/300], Train Loss: 0.0045\n",
      "Epoch [243/300], Train Loss: 0.0045\n",
      "Epoch [244/300], Train Loss: 0.0045\n",
      "Epoch [245/300], Train Loss: 0.0045\n",
      "Epoch [246/300], Train Loss: 0.0045\n",
      "Epoch [247/300], Train Loss: 0.0045\n",
      "Epoch [248/300], Train Loss: 0.0045\n",
      "Epoch [249/300], Train Loss: 0.0045\n",
      "Epoch [250/300], Train Loss: 0.0045\n",
      "Epoch [251/300], Train Loss: 0.0045\n",
      "Epoch [252/300], Train Loss: 0.0044\n",
      "Epoch [253/300], Train Loss: 0.0044\n",
      "Epoch [254/300], Train Loss: 0.0044\n",
      "Epoch [255/300], Train Loss: 0.0044\n",
      "Epoch [256/300], Train Loss: 0.0044\n",
      "Epoch [257/300], Train Loss: 0.0044\n",
      "Epoch [258/300], Train Loss: 0.0044\n",
      "Epoch [259/300], Train Loss: 0.0044\n",
      "Epoch [260/300], Train Loss: 0.0044\n",
      "Epoch [261/300], Train Loss: 0.0044\n",
      "Epoch [262/300], Train Loss: 0.0043\n",
      "Epoch [263/300], Train Loss: 0.0043\n",
      "Epoch [264/300], Train Loss: 0.0043\n",
      "Epoch [265/300], Train Loss: 0.0043\n",
      "Epoch [266/300], Train Loss: 0.0043\n",
      "Epoch [267/300], Train Loss: 0.0043\n",
      "Epoch [268/300], Train Loss: 0.0042\n",
      "Epoch [269/300], Train Loss: 0.0043\n",
      "Epoch [270/300], Train Loss: 0.0043\n",
      "Epoch [271/300], Train Loss: 0.0042\n",
      "Epoch [272/300], Train Loss: 0.0043\n",
      "Epoch [273/300], Train Loss: 0.0042\n",
      "Epoch [274/300], Train Loss: 0.0042\n",
      "Epoch [275/300], Train Loss: 0.0042\n",
      "Epoch [276/300], Train Loss: 0.0042\n",
      "Epoch [277/300], Train Loss: 0.0042\n",
      "Epoch [278/300], Train Loss: 0.0043\n",
      "Epoch [279/300], Train Loss: 0.0042\n",
      "Epoch [280/300], Train Loss: 0.0042\n",
      "Epoch [281/300], Train Loss: 0.0042\n",
      "Epoch [282/300], Train Loss: 0.0042\n",
      "Epoch [283/300], Train Loss: 0.0041\n",
      "Epoch [284/300], Train Loss: 0.0041\n",
      "Epoch [285/300], Train Loss: 0.0041\n",
      "Epoch [286/300], Train Loss: 0.0041\n",
      "Epoch [287/300], Train Loss: 0.0041\n",
      "Epoch [288/300], Train Loss: 0.0041\n",
      "Epoch [289/300], Train Loss: 0.0041\n",
      "Epoch [290/300], Train Loss: 0.0041\n",
      "Epoch [291/300], Train Loss: 0.0041\n",
      "Epoch [292/300], Train Loss: 0.0041\n",
      "Epoch [293/300], Train Loss: 0.0041\n",
      "Epoch [294/300], Train Loss: 0.0041\n",
      "Epoch [295/300], Train Loss: 0.0040\n",
      "Epoch [296/300], Train Loss: 0.0040\n",
      "Epoch [297/300], Train Loss: 0.0040\n",
      "Epoch [298/300], Train Loss: 0.0041\n",
      "Epoch [299/300], Train Loss: 0.0041\n",
      "Epoch [300/300], Train Loss: 0.0040\n",
      "\n",
      "검증 손실 (MSE): 0.0278\n",
      "실제값: [ 67.25888  123.75534    6.950478 817.0732   347.56097 ]\n",
      "예측값: [ 46.552773 130.22734   29.071873 770.06116  318.88403 ]\n",
      "\n",
      "다변수 학습 및 테스트 종료\n",
      "\n",
      "==================================================\n",
      "[다변수 학습 모델 평가 결과]\n",
      "소요된 시간: 242.96s\n",
      "0번째 변수에 대한 평가 결과\n",
      "RMSE: 0.1448\n",
      "Corr: 0.6010\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# 실험 2) 개선된 방식 - 전체 client 입력으로 학습 및 평가\n",
    "start_time = time.time()\n",
    "dataset = TimeSeriesDataset(\"./LD2011_2014.txt\", hyperparams[\"sequence_len\"], hyperparams[\"pred_length\"], hyperparams[\"sliding_window\"])\n",
    "print(\"다변수 학습 및 테스트 시작\")\n",
    "rmse, corr = train_evaluate(hyperparams, dataset)\n",
    "print(\"다변수 학습 및 테스트 종료\\n\")\n",
    "end_time = time.time()\n",
    "elapsed_time_all_clients = end_time - start_time\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"[다변수 학습 모델 평가 결과]\")\n",
    "print(f\"소요된 시간: {elapsed_time_all_clients:.2f}s\")\n",
    "\n",
    "print(f\"{client_num}번째 변수에 대한 평가 결과\")\n",
    "# 첫 변수만 추출\n",
    "rmse = rmse[..., 0]\n",
    "corr = corr[..., 0]\n",
    "\n",
    "print(f\"RMSE: {rmse.squeeze():.4f}\")\n",
    "print(f\"Corr: {corr.squeeze():.4f}\")\n",
    "print(\"=\" * 50)\n"
   ]
  },
  {
   "attachments": {
    "image-2.png": {
     "image/png": "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"
    },
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image-2.png](attachment:image-2.png)\n",
    "![image.png](attachment:image.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emotion-recognition",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
